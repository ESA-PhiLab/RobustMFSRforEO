{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules import SN7DataModule\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from src.datasources import S2_BANDS\n",
    "import torch\n",
    "from src.FSRCNN import FSRCNNModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(start=f\"2019-12-30\", end=f\"2020-01-31\")\n",
    "dm = SN7DataModule(date_range=date_range,s2_bands=S2_BANDS['true_color'],\n",
    "                   only_whole_scenes=True,\n",
    "                   batch_size=1, collate_2toN=True, normalize=True, standardize_sentinel=False)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FSRCNNModule(\n",
       "  (net): FSRCNN(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (shrink): Sequential(\n",
       "      (0): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (map): Sequential(\n",
       "      (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (expand): Sequential(\n",
       "      (0): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (deconv): ConvTranspose2d(64, 3, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4), output_padding=(1, 1))\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (registration_model): ShiftNet(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer5): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer8): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (drop1): Dropout(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (activ1): ReLU()\n",
       "    (fc2): Linear(in_features=1024, out_features=2, bias=False)\n",
       "  )\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = \"FSRCNN_SSIM\"\n",
    "net_args = Namespace(in_channels=3, out_channels=3, upscale_factor=2, additional_scaling=None)\n",
    "model = FSRCNNModule.load_from_checkpoint(\"gs://fdl_srhallucinate/models/sisr_FSRCNN_SSIM-epoch=7-step=7503.ckpt\", net_args=net_args,\n",
    "                                          shiftnet=True)\n",
    "\n",
    "# model = FSRCNNModule(net_args=net_args, shiftnet=True)\n",
    "# model = FSRCNNModule.load_from_checkpoint(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRResNetModule(\n",
       "  (net): Generator(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "      (1): PReLU(num_parameters=1)\n",
       "    )\n",
       "    (block2): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block3): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block4): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block5): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block6): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (prelu): PReLU(num_parameters=1)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block7): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block8): Sequential(\n",
       "      (0): UpsampleBLock(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (pixel_shuffle): PixelShuffle(upscale_factor=2)\n",
       "        (prelu): PReLU(num_parameters=1)\n",
       "      )\n",
       "      (1): Conv2d(64, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    )\n",
       "    (additional_scaling): Upsample(scale_factor=1.0469, mode=bicubic)\n",
       "  )\n",
       "  (registration_model): ShiftNet(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer5): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer6): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer7): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (layer8): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (drop1): Dropout(p=0.5, inplace=False)\n",
       "    (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (activ1): ReLU()\n",
       "    (fc2): Linear(in_features=1024, out_features=2, bias=False)\n",
       "  )\n",
       "  (loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO this SRGAN weights fail bc of renaming of generator\n",
    "# from src.SRGAN import SRResNetModule\n",
    "\n",
    "# modelname = \"FSRCNN_SRResNet\"\n",
    "# net_args = Namespace(in_channels=3, out_channels=3, upscale_factor=2, additional_scaling=None)\n",
    "# model = SRResNetModule.load_from_checkpoint(\"gs://fdl_srhallucinate/models/sisr_SRResNet_SSIM-epoch=5-step=5627.ckpt\", \n",
    "#                                             net_args=net_args,\n",
    "#                                             shiftnet=True)\n",
    "\n",
    "# # model = FSRCNNModule(net_args=net_args, shiftnet=True)\n",
    "# # model = FSRCNNModule.load_from_checkpoint(PATH)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "\n",
    "\n",
    "# # Load directly from the bucket works if installed gcsfs\n",
    "# checkpoint = pl_load(\"gs://fdl_srhallucinate/models/sisr_FSRCNN_SSIM-epoch=7-step=7503.ckpt\", \n",
    "#                      map_location=lambda storage, loc: storage)\n",
    "\n",
    "# model.load_state_dict(checkpoint['state_dict'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying predictions to: gs://fdl_srhallucinate/spacenet/predictions/FSRCNN_SSIM\n",
      "Dataloader val (0/5) making predictions for scene L15-0368E-1245N_1474_3210_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader val (1/5) making predictions for scene L15-0632E-0892N_2528_4620_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader val (2/5) making predictions for scene L15-1049E-1370N_4196_2710_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader val (3/5) making predictions for scene L15-1210E-1025N_4840_4088_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader val (4/5) making predictions for scene L15-1289E-1169N_5156_3514_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader test (0/4) making predictions for scene L15-0358E-1220N_1433_3310_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader test (1/4) making predictions for scene L15-0434E-1218N_1736_3318_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader test (2/4) making predictions for scene L15-0683E-1006N_2732_4164_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader test (3/4) making predictions for scene L15-0760E-0887N_3041_4643_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (0/37) making predictions for scene L15-0331E-1257N_1327_3160_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (1/37) making predictions for scene L15-0357E-1223N_1429_3296_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (2/37) making predictions for scene L15-0361E-1300N_1446_2989_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (3/37) making predictions for scene L15-0487E-1246N_1950_3207_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (4/37) making predictions for scene L15-0566E-1185N_2265_3451_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (5/37) making predictions for scene L15-0571E-1075N_2287_3888_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (6/37) making predictions for scene L15-0586E-1127N_2345_3680_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (7/37) making predictions for scene L15-0595E-1278N_2383_3079_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (8/37) making predictions for scene L15-0614E-0946N_2459_4406_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (9/37) making predictions for scene L15-0924E-1108N_3699_3757_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (10/37) making predictions for scene L15-0977E-1187N_3911_3441_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (11/37) making predictions for scene L15-1014E-1375N_4056_2688_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 489, 489]) output shape: torch.Size([1, 3, 978, 978]) highres shape: torch.Size([1, 1, 3, 1023, 1023])\n",
      "Dataloader train (12/37) making predictions for scene L15-1015E-1062N_4061_3941_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (13/37) making predictions for scene L15-1025E-1366N_4102_2726_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (14/37) making predictions for scene L15-1138E-1216N_4553_3325_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (15/37) making predictions for scene L15-1172E-1306N_4688_2967_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (16/37) making predictions for scene L15-1185E-0935N_4742_4450_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (17/37) making predictions for scene L15-1200E-0847N_4802_4803_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (18/37) making predictions for scene L15-1204E-1204N_4819_3372_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 489]) output shape: torch.Size([1, 3, 978, 978]) highres shape: torch.Size([1, 1, 3, 1023, 1023])\n",
      "Dataloader train (19/37) making predictions for scene L15-1209E-1113N_4838_3737_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (20/37) making predictions for scene L15-1276E-1107N_5105_3761_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (21/37) making predictions for scene L15-1296E-1198N_5184_3399_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (22/37) making predictions for scene L15-1298E-1322N_5193_2903_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (23/37) making predictions for scene L15-1335E-1166N_5342_3524_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (24/37) making predictions for scene L15-1389E-1284N_5557_3054_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (25/37) making predictions for scene L15-1438E-1134N_5753_3655_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (26/37) making predictions for scene L15-1439E-1134N_5759_3655_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader train (27/37) making predictions for scene L15-1479E-1101N_5916_3785_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (28/37) making predictions for scene L15-1481E-1119N_5927_3715_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader train (29/37) making predictions for scene L15-1538E-1163N_6154_3539_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (30/37) making predictions for scene L15-1617E-1207N_6468_3360_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 489, 490]) output shape: torch.Size([1, 3, 978, 980]) highres shape: torch.Size([1, 1, 3, 1023, 1024])\n",
      "Dataloader train (31/37) making predictions for scene L15-1672E-1207N_6691_3363_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 489]) output shape: torch.Size([1, 3, 980, 978]) highres shape: torch.Size([1, 1, 3, 1024, 1023])\n",
      "Dataloader train (32/37) making predictions for scene L15-1703E-1219N_6813_3313_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (33/37) making predictions for scene L15-1709E-1112N_6838_3742_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (34/37) making predictions for scene L15-1716E-1211N_6864_3345_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (35/37) making predictions for scene L15-1748E-1247N_6993_3202_13\n",
      "\t Input shape: torch.Size([1, 8, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Dataloader train (36/37) making predictions for scene L15-1848E-0793N_7394_5018_13\n",
      "\t Input shape: torch.Size([1, 16, 3, 490, 490]) output shape: torch.Size([1, 3, 980, 980]) highres shape: torch.Size([1, 1, 3, 1024, 1024])\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from src import pred_scenes\n",
    "\n",
    "scenes_preds_list = pred_scenes.predictions_to_bucket(model,name_prediction_folder=modelname,data_module=dm,verbose=True)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Image Collection in the GEE\n",
      "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&state=TcgAprbO43RPbQXaARUNdsbdV42oxO&prompt=consent&access_type=offline\n",
      "\n",
      "The authorization workflow will generate a code, which you should paste in the box below. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the authorization code: \n",
      " 4/1AY0e-g6IQGX5TwuwLJPijTC1KZtXLHq_jGy1nM60VNHfEUaktCvEdxiWMnQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Image Collection in GEE...: 100%|██████████| 46/46 [01:18<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create manually the name_prediction_folder in you GEE assets \n",
    "# pred_scenes.create_image_collection_gee(scenes_preds_list,name_prediction_folder=modelname,gee_user=\"gonzmg88\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading S2 images and Planet Images\n",
    "\n",
    "Info taken from:\n",
    "https://developers.google.com/earth-engine/Earth_Engine_asset_from_cloud_geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FSRCNN_SSIM'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../src/scene_split.json\",\"r\") as fh:\n",
    "    train_val_test_split = json.load(fh)\n",
    "train_val_test_split = train_val_test_split[\"split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "  'type': 'IMAGE',\n",
    "  'gcs_location': {\n",
    "    'uris': ['gs://ee-docs-demos/COG_demo.tif']\n",
    "  },\n",
    "  'properties': {\n",
    "    'split': 'train'\n",
    "  },\n",
    "  'startTime': '2016-01-01T00:00:00.000000000Z',\n",
    "  'endTime': '2016-12-31T15:01:23.000000000Z',\n",
    "}\n",
    "\n",
    "# Where Earth Engine assets are kept.\n",
    "url = 'https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/assets?assetId={}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_s2 = !gsutil ls gs://fdl_srhallucinate/spacenet/train/*/S2L2A/*.tif\n",
    "len(files_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Your user folder name and new asset name.\n",
    "asset_collection_name = f'users/{gee_username}/Spacenet7_S2'\n",
    "# TODO add cleareance score to add as a property\n",
    "\n",
    "for s2f in tqdm(files_s2):\n",
    "    date_s2 = os.path.splitext(os.path.basename(s2f))[0]\n",
    "    scene_id = os.path.basename(os.path.dirname(os.path.dirname(s2f)))\n",
    "    with rasterio.open(s2f) as rst:\n",
    "        cloud_mask = rst.read(13)\n",
    "    \n",
    "    clearance_mask = ((cloud_mask != 9) & (cloud_mask > 1))\n",
    "    clearance = float(np.mean(clearance_mask))\n",
    "    \n",
    "    start_time =   f'{date_s2}T00:00:00.000000000Z'\n",
    "    end_time = f'{date_s2}T23:59:00.000000000Z'\n",
    "    request_copy = request.copy()\n",
    "    request_copy['gcs_location']['uris'] = [s2f]\n",
    "    request_copy['startTime'] = start_time\n",
    "    request_copy['endTime'] = end_time\n",
    "    request_copy[\"properties\"] = {\"scene_id\": scene_id, \n",
    "                                  \"split\": train_val_test_split[scene_id],\n",
    "                                  \"clearance\": clearance}\n",
    "    \n",
    "    asset_name = f\"{scene_id}-{date_s2}\"\n",
    "    asset_id = f\"{asset_collection_name}/{asset_name}\"\n",
    "    response = session.post(\n",
    "        url = url.format(asset_id),\n",
    "        data = json.dumps(request_copy))\n",
    "\n",
    "    assert response.status_code == 200, f'{json.loads(response.content)}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_planet = !gsutil ls gs://fdl_srhallucinate/spacenet/train/*/imagesCOG/global_monthly_2020_01*.tif\n",
    "len(files_planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Your user folder name and new asset name.\n",
    "asset_collection_name = f'users/{gee_username}/Spacenet7Planet'\n",
    "\n",
    "for fpf in tqdm(files_planet):\n",
    "    basename_pred = os.path.splitext(os.path.basename(fpf))[0]\n",
    "    year, month= re.match(\"global_monthly_(\\d{4})_(\\d{2})_mosaic_\", basename_pred).groups()\n",
    "    \n",
    "    scene_id = os.path.basename(os.path.dirname(os.path.dirname(fpf)))\n",
    "    start_time =   f'{year}-{month}-01T00:00:00.000000000Z'\n",
    "    end_time = f'{year}-{month}-31T23:59:00.000000000Z'\n",
    "    request_copy = request.copy()\n",
    "    request_copy['gcs_location']['uris'] = [fpf]\n",
    "    request_copy['startTime'] = start_time\n",
    "    request_copy['endTime'] = end_time\n",
    "    request_copy[\"properties\"] = {\"scene_id\": scene_id, \"split\": train_val_test_split[scene_id]}\n",
    "    \n",
    "    asset_name = f\"{scene_id}-{year}-{month}\"\n",
    "    asset_id = f\"{asset_collection_name}/{asset_name}\"\n",
    "    \n",
    "    response = session.post(\n",
    "        url = url.format(asset_id),\n",
    "        data = json.dumps(request_copy))\n",
    "\n",
    "    assert response.status_code == 200, f'{json.loads(response.content)}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HighResNet",
   "language": "python",
   "name": "highresnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
